{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers huggingface_hub -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Approach 1.**\n",
    "\n",
    "Let's try to inference the whisper model on a test and manually specifying the conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-27T16:46:43.641317Z",
     "iopub.status.busy": "2025-10-27T16:46:43.640721Z",
     "iopub.status.idle": "2025-10-27T16:46:43.645341Z",
     "shell.execute_reply": "2025-10-27T16:46:43.644647Z",
     "shell.execute_reply.started": "2025-10-27T16:46:43.641293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "from transformers import pipeline\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from catboost import CatBoostClassifier\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoProcessor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSpeechSeq2Seq\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.auto import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Functions to inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T16:31:04.907946Z",
     "iopub.status.busy": "2025-10-27T16:31:04.907393Z",
     "iopub.status.idle": "2025-10-27T16:31:04.911857Z",
     "shell.execute_reply": "2025-10-27T16:31:04.911099Z",
     "shell.execute_reply.started": "2025-10-27T16:31:04.907925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_audio(path: str, target_sr: int = 16000):\n",
    "    \"\"\"\n",
    "    Read wav and resample to target_sr (if needed).\n",
    "    Returns numpy array (float32), mono.\n",
    "    \"\"\"\n",
    "    audio, sr = librosa.load(path, sr=target_sr, mono=True)\n",
    "    # librosa returns float32 in interval [-1,1]\n",
    "    return audio, target_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T16:31:04.913300Z",
     "iopub.status.busy": "2025-10-27T16:31:04.912701Z",
     "iopub.status.idle": "2025-10-27T16:31:04.931213Z",
     "shell.execute_reply": "2025-10-27T16:31:04.930481Z",
     "shell.execute_reply.started": "2025-10-27T16:31:04.913282Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # your device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T16:31:04.932896Z",
     "iopub.status.busy": "2025-10-27T16:31:04.932717Z",
     "iopub.status.idle": "2025-10-27T16:31:04.948652Z",
     "shell.execute_reply": "2025-10-27T16:31:04.947891Z",
     "shell.execute_reply.started": "2025-10-27T16:31:04.932881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def transcribe_wav(\n",
    "    wav_path: str,\n",
    "    model_id: str = \"openai/whisper-large-v3-turbo\",\n",
    "    device: int | str | None = None,\n",
    "    chunk_length_s: float | None = None,\n",
    "    **pipeline_kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    transcribe_wav(...) -> dict\n",
    "    Returns pipeline's response (usually dict with 'text' etc.).\n",
    "    If chunk_length_s — audio will split on batches and transcribate in order.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "    asr = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model_id,\n",
    "        device=device,\n",
    "        **pipeline_kwargs\n",
    "    )\n",
    "\n",
    "    \n",
    "    audio, sr = load_audio(wav_path, target_sr=16000)\n",
    "\n",
    "    if not chunk_length_s:\n",
    "        out = asr(audio, chunk_length_s=None)\n",
    "        return out\n",
    "\n",
    "    samples_per_chunk = int(chunk_length_s * sr)\n",
    "    texts = []\n",
    "    for i in range(0, len(audio), samples_per_chunk):\n",
    "        chunk = audio[i : i + samples_per_chunk]\n",
    "        if len(chunk) == 0:\n",
    "            continue\n",
    "        chunk_out = asr(chunk, chunk_length_s=None)\n",
    "        if isinstance(chunk_out, dict):\n",
    "            texts.append(chunk_out.get(\"text\", \"\").strip())\n",
    "        else:\n",
    "            texts.append(str(chunk_out).strip())\n",
    "    full_text = \" \".join(t for t in texts if t)\n",
    "    return {\"text\": full_text}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Try to use it in one example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T16:31:08.159742Z",
     "iopub.status.busy": "2025-10-27T16:31:08.158931Z",
     "iopub.status.idle": "2025-10-27T16:31:12.679141Z",
     "shell.execute_reply": "2025-10-27T16:31:12.678290Z",
     "shell.execute_reply.started": "2025-10-27T16:31:08.159713Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "`return_token_timestamps` is deprecated for WhisperFeatureExtractor and will be removed in Transformers v5. Use `return_attention_mask` instead, as the number of frames can be inferred from it.\n",
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Дату указываю внизу. Мы с вами встретимся\n"
     ]
    }
   ],
   "source": [
    "WAV_PATH = \"/kaggle/input/vseross-2-audio/wav_test/0000219778122723066859323624505982384475.wav\"\n",
    "result = transcribe_wav(WAV_PATH, device=device)\n",
    "text = result.get(\"text\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Well, so, now we gonna try run this code in all samples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to change this function to inference in all samples in folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T16:39:44.943118Z",
     "iopub.status.busy": "2025-10-27T16:39:44.942839Z",
     "iopub.status.idle": "2025-10-27T16:39:44.950629Z",
     "shell.execute_reply": "2025-10-27T16:39:44.949916Z",
     "shell.execute_reply.started": "2025-10-27T16:39:44.943099Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def transcribe_wav(\n",
    "    audio_path: str,\n",
    "    model_id: str = \"openai/whisper-large-v3-turbo\",\n",
    "    device: int | str | None = None,\n",
    "    chunk_length_s: float | None = None,\n",
    "    **pipeline_kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    transcribe_wav(...) -> dict\n",
    "    Returns pipeline's response (usually dict with 'text' etc.).\n",
    "    If chunk_length_s — audio will split on batches and transcribate in order.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "    asr = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model_id,\n",
    "        device=device,\n",
    "        **pipeline_kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "    data = []\n",
    "    for wav_path in tqdm(os.listdir(audio_path), desc='Inferencing'):\n",
    "        wav_path = Path(audio_path) / wav_path\n",
    "        audio, sr = load_audio(wav_path, target_sr=16000)\n",
    "    \n",
    "        samples_per_chunk = int(chunk_length_s * sr)\n",
    "        texts = []\n",
    "        for i in range(0, len(audio), samples_per_chunk):\n",
    "            chunk = audio[i : i + samples_per_chunk]\n",
    "            if len(chunk) == 0:\n",
    "                continue\n",
    "            chunk_out = asr(chunk, chunk_length_s=None)\n",
    "            if isinstance(chunk_out, dict):\n",
    "                texts.append(chunk_out.get(\"text\", \"\").strip())\n",
    "            else:\n",
    "                texts.append(str(chunk_out).strip())\n",
    "        full_text = \" \".join(t for t in texts if t)\n",
    "        data.append({\n",
    "            \"path\": wav_path,\n",
    "            \"text\": full_text\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T16:39:55.007099Z",
     "iopub.status.busy": "2025-10-27T16:39:55.006729Z",
     "iopub.status.idle": "2025-10-27T16:39:55.010692Z",
     "shell.execute_reply": "2025-10-27T16:39:55.009861Z",
     "shell.execute_reply.started": "2025-10-27T16:39:55.007075Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "app_1 = transcribe_wav(audio_path=\"/kaggle/input/vseross-2-audio/wav_test\", device=device, chunk_length_s=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "app_1['target'] = app_1['text'].apply(lambda x: 1 if 'не слыш' in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "app_1.to_csv('approach_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Approach 2.**\n",
    "\n",
    "So, in the previous step we've used:\n",
    "```py\n",
    "app_1['text'].apply(lambda x: 1 if 'не слыш' in x else 0)\n",
    "```\n",
    "But it ain't so accurately. We can try to train catboost to classify texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df = transcribe_wav(audio_path=\"/kaggle/input/vseross-2-audio/wav_train\", device=device, chunk_length_s=4000)\n",
    "test_df = app_1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T16:44:52.509773Z",
     "iopub.status.busy": "2025-10-27T16:44:52.508499Z",
     "iopub.status.idle": "2025-10-27T16:44:52.642036Z",
     "shell.execute_reply": "2025-10-27T16:44:52.641226Z",
     "shell.execute_reply.started": "2025-10-27T16:44:52.509744Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open('/kaggle/input/train-json-audio-vseross/word_bounds.json', 'rb') as file:\n",
    "    word_bounds = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T16:45:38.708384Z",
     "iopub.status.busy": "2025-10-27T16:45:38.708085Z",
     "iopub.status.idle": "2025-10-27T16:45:38.713171Z",
     "shell.execute_reply": "2025-10-27T16:45:38.712421Z",
     "shell.execute_reply.started": "2025-10-27T16:45:38.708364Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trues = list(word_bounds.keys()) # get targets for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df['target'] = train_df['path'].apply(lambda x: 1 if x.split('/')[-1].split('.')[0] in trues else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train catboost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = train_df[['text']]\n",
    "y = train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cbm = CatBoostClassifier(\n",
    "    10000,\n",
    "    text_features=['text'],\n",
    "    eval_metric='Accuracy',\n",
    "    verbose=1000,\n",
    ")\n",
    "cbm.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df['target'] = cbm.predict(test_df[['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df.to_csv('approach_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Approach 3.**\n",
    "\n",
    "So, it's a maximum score we could do with the previous approach without training. Now we gonna train whisper-medium on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T16:51:09.417354Z",
     "iopub.status.busy": "2025-10-27T16:51:09.416607Z",
     "iopub.status.idle": "2025-10-27T16:51:09.421055Z",
     "shell.execute_reply": "2025-10-27T16:51:09.420159Z",
     "shell.execute_reply.started": "2025-10-27T16:51:09.417330Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_audio_dir = '/kaggle/input/vseross-2-audio/wav_train'\n",
    "train_json_file = '/kaggle/input/train-json-audio-vseross/word_bounds.json'\n",
    "\n",
    "test_audio_dir = '/kaggle/input/vseross-2-audio/wav_test'\n",
    "\n",
    "model_id = \"openai/whisper-medium\"\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 4\n",
    "\n",
    "n_splits = 5\n",
    "num_epochs = 1\n",
    "\n",
    "encoder_lr = 1e-5\n",
    "head_lr = 3e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(train_json_file) as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "train_df = pd.DataFrame({'id': [file_name[:file_name.rfind('.')] for file_name in os.listdir(train_audio_dir)]})\n",
    "train_df['label'] = train_df['id'].apply(lambda id: id in train_data).astype(int)\n",
    "train_df['audio_path'] = train_audio_dir + train_df['id'] + '.wav'\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "splitter = StratifiedKFold(n_splits=n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClassificationDataset(Dataset):\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "    \n",
    "    def __init__(self, audio_paths, labels=None, sampling_rate=16_000):\n",
    "        self.audio_paths = audio_paths\n",
    "        self.labels = labels\n",
    "        self.sampling_rate = sampling_rate\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_paths[idx]\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        if sr != self.sampling_rate:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, self.sampling_rate)\n",
    "\n",
    "        inputs = self.processor(\n",
    "            waveform.squeeze(0),\n",
    "            sampling_rate=self.sampling_rate,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        inputs_dict = {\"input_features\": inputs.input_features.squeeze(0)}\n",
    "        if self.labels is not None:\n",
    "            inputs_dict[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        return inputs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModelForSpeechSeq2Seq.from_pretrained(model_id).model.encoder  # только энкодер\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.encoder.layer_norm.normalized_shape[0], 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_features, labels=None):\n",
    "        outputs = self.encoder(input_features)\n",
    "        hidden_states = outputs.last_hidden_state  # (batch, seq_len, hidden)\n",
    "        \n",
    "        # усредняем по времени (глобальный пуллинг)\n",
    "        pooled = hidden_states.mean(dim=1)\n",
    "        logits = self.head(pooled)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save\n",
    "models = []\n",
    "fit_results = []\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dtype = torch.float32\n",
    "\n",
    "# Train\n",
    "for i, (train_index, valid_index) in enumerate(splitter.split(train_df, train_df['label'])):\n",
    "    print(f\"Split: {i + 1}\", end=\"\\n\\n\")\n",
    "\n",
    "    train_audio_paths_split, train_labels_split = train_df['audio_path'].values[train_index], train_df['label'].values[train_index]\n",
    "    valid_audio_paths_split, valid_labels_split = train_df['audio_path'].values[valid_index], train_df['label'].values[valid_index]\n",
    "\n",
    "    train_dataset = AudioClassificationDataset(train_audio_paths_split, train_labels_split)\n",
    "    valid_dataset = AudioClassificationDataset(valid_audio_paths_split, valid_labels_split)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        num_workers=num_workers, \n",
    "        shuffle=True, \n",
    "        drop_last=True,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size=batch_size, \n",
    "        num_workers=num_workers, \n",
    "        shuffle=False, \n",
    "        drop_last=False,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    model = AudioClassificationModel()\n",
    "    model.to(device=device, dtype=dtype)\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.encoder.parameters(), 'lr': encoder_lr},\n",
    "        {'params': model.head.parameters(), 'lr': head_lr}\n",
    "    ])\n",
    "    \n",
    "    fit_result = {\n",
    "        'train_losses': [],\n",
    "        'train_f1': [],\n",
    "        'valid_losses': [],\n",
    "        'valid_f1': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # === TRAIN ===\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        all_train_preds, all_train_targets = [], []\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            input_features = batch['input_features'].to(device=device, dtype=dtype)\n",
    "            labels = batch['labels'].to(device=device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_features, labels=labels)\n",
    "            loss = outputs['loss']\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = outputs['logits'].argmax(dim=1).detach().cpu().numpy()\n",
    "            targets = labels.cpu().numpy()\n",
    "\n",
    "            all_train_preds.extend(preds)\n",
    "            all_train_targets.extend(targets)\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        train_loss = sum(train_losses) / len(train_losses)\n",
    "        train_f1 = f1_score(all_train_targets, all_train_preds, average='macro')\n",
    "        fit_result['train_losses'].append(train_loss)\n",
    "        fit_result['train_f1'].append(train_f1)\n",
    "        print(f\"Train loss: {train_loss:.4f} | F1 (macro): {train_f1:.4f}\")\n",
    "\n",
    "        # === VALID ===\n",
    "        model.eval()\n",
    "        valid_losses = []\n",
    "        all_valid_preds, all_valid_targets = [], []\n",
    "\n",
    "        progress_bar = tqdm(valid_loader, desc=f\"Epoch {epoch} [Valid]\")\n",
    "        with torch.no_grad():\n",
    "            for batch in progress_bar:\n",
    "                input_features = batch['input_features'].to(device=device, dtype=dtype)\n",
    "                labels = batch['labels'].to(device=device)\n",
    "\n",
    "                outputs = model(input_features, labels=labels)\n",
    "                loss = outputs['loss']\n",
    "\n",
    "                preds = outputs['logits'].argmax(dim=1).detach().cpu().numpy()\n",
    "                targets = labels.cpu().numpy()\n",
    "\n",
    "                all_valid_preds.extend(preds)\n",
    "                all_valid_targets.extend(targets)\n",
    "                valid_losses.append(loss.item())\n",
    "\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        valid_loss = sum(valid_losses) / len(valid_losses)\n",
    "        valid_f1 = f1_score(all_valid_targets, all_valid_preds, average='macro')\n",
    "\n",
    "        fit_result['valid_losses'].append(valid_loss)\n",
    "        fit_result['valid_f1'].append(valid_f1)\n",
    "\n",
    "        print(f\"Valid loss: {valid_loss:.4f} | F1 (macro): {valid_f1:.4f}\\n\")\n",
    "\n",
    "    fit_results.append(fit_result)\n",
    "    models.append(model)\n",
    "    \n",
    "    path = f'models/split_{i + 1}.pt'\n",
    "    torch.save(model.state_dict(), path)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Inference on test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTestDataset(Dataset):\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "    def __init__(self, audio_paths, sampling_rate=16_000):\n",
    "        self.audio_paths = audio_paths\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_paths[idx]\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        if sr != self.sampling_rate:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, self.sampling_rate)\n",
    "\n",
    "        inputs = self.processor(\n",
    "            waveform.squeeze(0),\n",
    "            sampling_rate=self.sampling_rate,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\"input_features\": inputs.input_features.squeeze(0), \"id\": os.path.basename(audio_path).replace('.wav','')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dtype = torch.float32\n",
    "model = AudioClassificationModel()\n",
    "model.to(device=device, dtype=dtype)\n",
    "model.load_state_dict(torch.load('models/split_1.pt', map_location=device))\n",
    "model.to(device, dtype=dtype)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size_test = 8\n",
    "\n",
    "test_audio_paths = [os.path.join(test_audio_dir, f) for f in os.listdir(test_audio_dir) if f.endswith('.wav')]\n",
    "test_loader = DataLoader(AudioTestDataset(test_audio_paths),\n",
    "                         batch_size=batch_size_test, shuffle=False,\n",
    "                         num_workers=num_workers, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Test Inference\"):\n",
    "        input_features = batch['input_features'].to(device=device, dtype=dtype)\n",
    "        outputs = model(input_features)\n",
    "        # DataParallel: берем mean по GPU, если есть\n",
    "        logits = outputs['logits']\n",
    "        preds = logits.argmax(dim=1).detach().cpu().numpy()\n",
    "\n",
    "        all_preds.extend(preds)\n",
    "        all_ids.extend(batch['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id': all_ids,\n",
    "    'label': all_preds\n",
    "})\n",
    "\n",
    "submission.to_csv(f'approach_3_{model_id}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Approach 4.**\n",
    "\n",
    "Pretty well, previous approach was the best. So, now we'll try to train some models and then to blend them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train other models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_id in ['openai/whisper-large-v3-turbo', 'openai/whisper-small-v2']:\n",
    "    print(f\"Model: {model_id}\")\n",
    "\n",
    "    models = []\n",
    "    fit_results = []\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dtype = torch.float32\n",
    "\n",
    "    # Train\n",
    "    for i, (train_index, valid_index) in enumerate(splitter.split(train_df, train_df['label'])):\n",
    "        print(f\"Split: {i + 1}\", end=\"\\n\\n\")\n",
    "\n",
    "        train_audio_paths_split, train_labels_split = train_df['audio_path'].values[train_index], train_df['label'].values[train_index]\n",
    "        valid_audio_paths_split, valid_labels_split = train_df['audio_path'].values[valid_index], train_df['label'].values[valid_index]\n",
    "\n",
    "        train_dataset = AudioClassificationDataset(train_audio_paths_split, train_labels_split)\n",
    "        valid_dataset = AudioClassificationDataset(valid_audio_paths_split, valid_labels_split)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            num_workers=num_workers, \n",
    "            shuffle=True, \n",
    "            drop_last=True,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset, \n",
    "            batch_size=batch_size, \n",
    "            num_workers=num_workers, \n",
    "            shuffle=False, \n",
    "            drop_last=False,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        model = AudioClassificationModel()\n",
    "        model.to(device=device, dtype=dtype)\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': model.encoder.parameters(), 'lr': encoder_lr},\n",
    "            {'params': model.head.parameters(), 'lr': head_lr}\n",
    "        ])\n",
    "        \n",
    "        fit_result = {\n",
    "            'train_losses': [],\n",
    "            'train_f1': [],\n",
    "            'valid_losses': [],\n",
    "            'valid_f1': []\n",
    "        }\n",
    "\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            # === TRAIN ===\n",
    "            model.train()\n",
    "            train_losses = []\n",
    "            all_train_preds, all_train_targets = [], []\n",
    "            progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\")\n",
    "            \n",
    "            for batch in progress_bar:\n",
    "                input_features = batch['input_features'].to(device=device, dtype=dtype)\n",
    "                labels = batch['labels'].to(device=device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(input_features, labels=labels)\n",
    "                loss = outputs['loss']\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                preds = outputs['logits'].argmax(dim=1).detach().cpu().numpy()\n",
    "                targets = labels.cpu().numpy()\n",
    "\n",
    "                all_train_preds.extend(preds)\n",
    "                all_train_targets.extend(targets)\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "            train_loss = sum(train_losses) / len(train_losses)\n",
    "            train_f1 = f1_score(all_train_targets, all_train_preds, average='macro')\n",
    "            fit_result['train_losses'].append(train_loss)\n",
    "            fit_result['train_f1'].append(train_f1)\n",
    "            print(f\"Train loss: {train_loss:.4f} | F1 (macro): {train_f1:.4f}\")\n",
    "\n",
    "            # === VALID ===\n",
    "            model.eval()\n",
    "            valid_losses = []\n",
    "            all_valid_preds, all_valid_targets = [], []\n",
    "\n",
    "            progress_bar = tqdm(valid_loader, desc=f\"Epoch {epoch} [Valid]\")\n",
    "            with torch.no_grad():\n",
    "                for batch in progress_bar:\n",
    "                    input_features = batch['input_features'].to(device=device, dtype=dtype)\n",
    "                    labels = batch['labels'].to(device=device)\n",
    "\n",
    "                    outputs = model(input_features, labels=labels)\n",
    "                    loss = outputs['loss']\n",
    "\n",
    "                    preds = outputs['logits'].argmax(dim=1).detach().cpu().numpy()\n",
    "                    targets = labels.cpu().numpy()\n",
    "\n",
    "                    all_valid_preds.extend(preds)\n",
    "                    all_valid_targets.extend(targets)\n",
    "                    valid_losses.append(loss.item())\n",
    "\n",
    "                    progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "            valid_loss = sum(valid_losses) / len(valid_losses)\n",
    "            valid_f1 = f1_score(all_valid_targets, all_valid_preds, average='macro')\n",
    "\n",
    "            fit_result['valid_losses'].append(valid_loss)\n",
    "            fit_result['valid_f1'].append(valid_f1)\n",
    "\n",
    "            print(f\"Valid loss: {valid_loss:.4f} | F1 (macro): {valid_f1:.4f}\\n\")\n",
    "\n",
    "        fit_results.append(fit_result)\n",
    "        models.append(model)\n",
    "        \n",
    "        path = f'models/split_{i + 1}_{model_id}.pt'\n",
    "        torch.save(model.state_dict(), path)\n",
    "        break\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dtype = torch.float32\n",
    "    model = AudioClassificationModel()\n",
    "    model.to(device=device, dtype=dtype)\n",
    "    model.load_state_dict(torch.load(f'models/split_1_{model_id}.pt', map_location=device))\n",
    "    model.to(device, dtype=dtype)\n",
    "    model.eval();\n",
    "\n",
    "    all_preds = []\n",
    "    all_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Test Inference\"):\n",
    "            input_features = batch['input_features'].to(device=device, dtype=dtype)\n",
    "            outputs = model(input_features)\n",
    "            # DataParallel: берем mean по GPU, если есть\n",
    "            logits = outputs['logits']\n",
    "            preds = logits.argmax(dim=1).detach().cpu().numpy()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_ids.extend(batch['id'])\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': all_ids,\n",
    "        'label': all_preds\n",
    "    })\n",
    "\n",
    "    submission.to_csv(f'approach_3_{model_id}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Blending**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_1 = pd.read_csv('approach_3_openai/whisper-medium.csv')\n",
    "sub_2 = pd.read_csv('approach_3_openai/whisper-large-v3-turbo.csv')\n",
    "sub_3 = pd.read_csv('approach_3_openai/whisper-small-v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = sub_1.drop(columns=['label']).copy()\n",
    "sub['label1'] = sub_1['label']\n",
    "sub['label2'] = sub_2['label']\n",
    "sub['label3'] = sub_3['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for i, row in sub.iterrows():\n",
    "    votes = [row['label1'], row['label2'], row['label3']]\n",
    "    final_label = max(set(votes), key=votes.count)  # majority vote\n",
    "    preds.append(final_label)\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    'id': sub['id'],\n",
    "    'label': preds\n",
    "})\n",
    "sub.to_csv('approach_4_blended.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8491427,
     "sourceId": 13383041,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8507674,
     "sourceId": 13405709,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
